\chapter{Conclusion}
\label{chapter:conclusion}

In this chapter we synthesise the multiple contributions of this manuscript. In \Cref{sec:contribs}, we recall the several limitations of the current approaches for \acrfullpl{DS} \idx{model} and we expose how we tackle those limitations. Then we propose insights to improve or even extend each of those contributions. Finally, in \Cref{sec:perspective}, we foresight the state of art of further works on dialogue systems and user adaptation.

\section{Contributions}
\label{sec:contribs}

In \Cref{chapter:introduction}, we saw that \gls{DS} applications are booming. In particular, this is the expansion of individual smart phones, TVs and car that makes the research in \gls{DS} very attractive from the industrial point of view. 
The current \gls{DS} technologies in industrial systems mostly rely on \idx{handcrafted} strategies, but research's state of the art is now based on statistical strategies. For the latter, the \acrfull{DM}, defined as the decision making component of a \gls{DS} (\Cref{chapter:the-pipeline}), is usually cast as an \acrfull{RL} problem (\Cref{chapter:dm-rl}). The strategy is trained from conversational data, improving efficiency, robustness, and facilitating in the meantime the work of the designer. One of the limitations this approach faces is that the \idx{dialogue corpus} mixes various \idx{user}s records, hence a system that trains on this corpus may adopt a too generic strategy.

To overcome this issue, we propose a solution based on \acrfull{TL} (\Cref{chapter:dm-tl}), where we consider a pool of different \idx{dialogue corpora} corresponding to several \idx{user}s (\Cref{chapter:sigdial}). The novelty proposed in this work is to introduce a way to cluster the \idx{dialogue corpora} using the similarities in the behaviour of their respective \gls{DS}. That way, we can elect representative \glspl{DS} of the whole database. That allows us to plug the \idx{user adaptation} framework proposed in \textcite{Genevay2016} to operate \idx{Online} \gls{TL}. Despite good results with \idx{handcrafted user}s, our experiments suffer from the fact that the toy game we experiment on and the models\index{model} used are too simple to extract discriminative behaviours among human-model users\index{human-model user}. One of our further work could consider more challenging \idx{dialogue simulator}s as for example the complex \acrlong{NDG}~\parencite{laroche2017ndg}. Another path of research could be a direct experiment with real users\index{user} instead of human-model users\index{human-model user}.

With the recent spate of \acrfull{DRL}, the idea of incorporating \gls{TL} is tempting.
This is the reason why, in \Cref{sec:continuous}, %!TU: avoid short forms in thesis
we propose to improve the \gls{DQN} algorithm to include a transfer mechanism.

%!TU: A mon avis le passe (we tried) aurait ete plus logique en conclusion
% mais OK va pour le present...
% Mais alors que dire de la premiere phrase de cette section qui est au passe ?
%NC j'avais mis tout au passé à la base, et on m'a dit de mettre au présent
We try several solutions to setup the \textit{knowledge-transfer} phase and the \textit{learning} phase with no sucess so far. However, we believe that extending \gls{DQN} with \gls{TL} is of high interest as the dialogue system should continuously improve itself with recurrent users and use previous knowledge to handle a growing base of new users. The bottleneck of this approach is, as the procedure is online, the information about the environment is partial but growing. So, one should find a way to pick the right policy/transitions given the current knowledge of the environment. Also, using the current policy as a comparison sample for the \textit{knowledge-transfer} phase is of limited interest as the policy is currently bootstrapping.
%!TU: c'est la que l'on se dit que le cadre formel des POMDP avec leurs variables latentes et leurs beliefs sont parfaits pour le TL...
% Mais on en discutera autour d'un verre apres ta soutenance ^_^
% A moins que la question ne te soit posee pendant l'expose ?
%
Another limitation of current \gls{RL} approaches for \gls{DS} is that the very first interactions with a fresh known \idx{user} are poorly handled. One solution could be using pre-learnt \acrfull{DP} as in \Cref{chapter:sigdial}. We come up with another idea. In \Cref{chapter:nips} and \Cref{chapter:slsp}, we consider the dialogue process as a safe \gls{RL} problem where the critical aspect of the \idx{dialogue} is when a \idx{user} hangs up\index{hang-up}. The idea is then to use a generic, but \idx{safe policy}, as a proxy to the optimal \gls{DS}, in the first interactions with the new \idx{user}. This work is divided in two parts.

First, in \Cref{chapter:nips}, we introduce a new \idx{batch} \gls{RL} algorithm to learn budgeted policies\index{budgeted policy}. We show that those policies managed to contain the \idx{hangup} frequencies of \idx{user}s bellow a chosen threshold and could be good candidates for proxies. If this approach shown promising results, and theoretical optimality, it lacks of theoretical convergence. We show that it cannot be achieved in the general case. However, we believe that if we restrict the form of the $\Q$-function to a certain form of smoothness actually encountered in the experiments, we can achieve convergence. So the next line of research would involve a deeper theoretical analysis of this convergence. Another issue raised by the algorithm is that it achieves optimally for a restricted class of budgeted policies\index{budgeted policy}. Indeed, compared to classical \gls{RL}, the \idx{agent} has two degrees of freedom, it can choose the action distribution, and the associated budgets\index{budget}. The problem is, for any next state, the \idx{budget} given will be the same. We believe that being able to control the \idx{budget} the next state receives is crucial to achieve optimality over any class of policies\index{policy}. This idea is already explored in \parencite{Boutilier_Lu:uai16} but restricted to known and tractable \idx{environment} and thus cannot be applied for \glspl{DS}.

In a second time, in \Cref{chapter:slsp}, we introduce an actual method to \idx{transfer} safe policies. In order to validate the approach in the simplest context we can achieve, instead of learning budgeted policies\index{budgeted policy} as proposed in \Cref{chapter:nips}, we design \idx{handcrafted} policies\index{policy} equipped with different levels of safety in their behaviour. Then, we propose to \idx{transfer} those policies using the same idea of \idx{$\egreedy$-greedy} when the \idx{agent} alternates between a safe, an exploratory and a \idx{greedy} \idx{policy}. Transferring\index{transfer} safe policies does not show significant advantage over \idx{transfer}ring regular policies. We believe that the \idx{hangup-model}, based on an uniform distribution,
is too simple,
%! TU: past tense ???
so we plan to design the \idx{hangup-model} with a Poisson distribution. We also want to replace the \idx{handcrafted} policies by actual \gls{RL} policies learned on source \idx{user}s using \idx{Lagrangian relaxation} or even \gls{BFTQ}. Finally, we may consider a real application on the \acrlong{DSTC} corpus. %!TU: direct or indirect style ?? il faut etre coherent. "we may consider..."

\section{On the long run}
\label{sec:perspective}

The most breathtaking milestones achieved in \gls{RL}  primarily include Deep-Learning solutions. A lot of work has been done for games: OpenAI five~\parencite{OpenAIdota} managed to win against the world champion team in  Dota 2, a highly competitive Multiplayer Online Battle Arena; DeepMind AlphaGO beat the world champion of GO~\parencite{alphago}, while AlphaStar~\parencite{alphastar} mastered the Real Time Strategy game StarCraft II. While those approaches rely on high-end engineering solutions and an infinity of simulation data, the future of \gls{AI} should focus on end-to-end approaches with no supervision nor plug-n-play simulators (as for real dialogues).
%!TU: these game environments can provide an infinity of simulation data, which is not the case for real dialogs
The first step in this direction has been done by OpenAI with GPT-2~\parencite{radford2019language}, a Transformer language model~\parencite{radford2019language} so good that the authors did not release the full model afraid of potential malicious usages.  We emphasis on the fact that this model requires no supervision (it is an unsupervised learning algorithm) using attention mechanisms. A very recent attempt tend to incorporate such language models into task-oriented \glspl{DS}~\parencite{gpt2taskoriented} and we believe that should be the future of \glspl{DS}: end-to-end \glspl{DS}~\parencite{serban2016building,li2017end}.

Those \glspl{DS} will do very well for most of the situation, but will probably fail at user adaptation as those end-to-end approaches do not incorporate a mechanism to differentiate the environments and thus the users. User adaptation will greatly benefit from the recent advances in the robotics field, as one of the main obstacles in \gls{RL} for robotic is how to transfer the knowledge from the simulated environment to the real environment. We notice an upsurge of work on this subject~\parencite{kang2019generalization,uav} and this should continue with the expansion of autonomous driving cars and Unmanned Aerial Vehicules. That being said, the next step in user adaptation should focus on co-adaptation. When dealing with humans, we must expect the user to adapt  to the \gls{DS}. It is particularly true when we want a system dedicated to a specific user as, for example, a vocal assistant. For the moment, the problem has been tackled by a few~\parencite{chandramohan2014co,barlier2015human}.

The last research lead, but not least, should focus on the safety of \glspl{DS}. In this thesis, one of the first stones was laid, but a lot remains to do. This component is essential to production \glspl{DS} as nobody wants to face, for instance, a very rude vocal assistant~\parencite{techcrunch}.