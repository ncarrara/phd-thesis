\documentclass[french,handout]{beamer}
\usetheme[secheader]{Boadilla}
\usecolortheme[named=orange]{structure}
\usepackage[utf8]{inputenc}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{tabularx}
\usepackage{makecell}


\usefonttheme{serif}
\usepackage{xkeyval}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[acronym,toc,symbols]{glossaries}
\usepackage{subfig}
\usepackage[block=space,style=authoryear,citestyle=authoryear,sorting=nyt,sortcites=false,autopunct=true,autolang=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber,maxcitenames=1,maxbibnames=99]{biblatex}
\usepackage{diagbox}
%\usepackage[block=space,style=authoryear,citestyle=authoryear,sorting=nyt,sortcites=true,autopunct=true,autolang=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
%\usepackage[backend=biber]{biblatex}
\definecolor{macouleur}{rgb}{0.75,0.43,0.09}
\AtBeginSection[]{
{
%\setbeamercolor{background canvas}{bg=macouleur}
\begin{frame}
    \vfill
    \centering
    \begin{beamercolorbox}[sep=15pt,center,shadow=true,rounded=true]{title}
        %\usebeamerfont{title}\insertsectionhead\par
        %\frametitle{Outline for section \thesection}
        \tableofcontents[currentsection]
    \end{beamercolorbox}
    \vfill
\end{frame}
}
}
\AtBeginSubsection[]{
{
\begin{frame}
    \vfill
    \centering
    \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
        \usebeamerfont{title}\insertsubsectionhead\par
        %
    \end{beamercolorbox}
    \vfill
\end{frame}
}
}

\addbibresource{../bibliography.bib} % BibTeX bibliography file
\defbibheading{bibempty}{}

\input{../mathdef.tex}
%\input{../glossary.tex}
\input{symbol.tex}

\presetkeys{todonotes}{inline}{}
\title[TITLE]{Apprentissage par renforcement pour l'optimisation des systèmes de dialogue via l'adaptation à l'utilisateur.}
\subtitle{Soutenance de thèse.}
\author{Nicolas Carrara}
\institute[ULille]{Université de Lille}
\date{Le 18 Décembre 2019}

\setbeamertemplate{footline}{%
\hfill%
\usebeamercolor[fg]{page number in head/foot}%
\usebeamerfont{page number in head/foot}%
\insertframenumber%
%\,/\,\inserttotalframenumber
\kern1em\vskip2pt%
}

\beamertemplatenavigationsymbolsempty

\begin{document}

    \begin{frame}
        \maketitle
        \centering
    \end{frame}

    \section{Introduction}

    \begin{frame}{Les sytèmes de dialogues}

        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{drawio/out/sds.pdf}
                % Un Anneau pour les gouverner tous
            \end{center}
        \end{figure}

    \end{frame}

    \begin{frame}{Trois types d'applications}

        \begin{itemize}
            \item Discussion social sans but précis.
            \item Question-Réponse
            \item \textbf{Dialogues orientés tâches.}
        \end{itemize}

        %C'est surtout la troisième problématique qui nous interesse.

    \end{frame}

    \begin{frame}{\textbf{Systèmes de dialogue} | sans adaptation}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.6\textwidth]{img/adap0.pdf}
                % Un Anneau pour les gouverner tous
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Systèmes de dialogue} | avec adaptation}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.6\textwidth]{img/adap1.pdf}
                % comment savoir quel système utiliser ?
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{Personalisation}

        % avec l'essort du smartphone/car/etc
        De plus en plus de systèmes deviennent personnels

        \begin{itemize}
            \item Assistants personnels % par définition personnels
            \item Maisons connectées % parler à l'enfant, les parents etc (les enfants sont moins succeptible de demander certain choses etc)
            \item Voitures connectées
            % handfree gps etc
        \end{itemize}

    \end{frame}

    \begin{frame}{Problématique}

        On pourrait discriminer deux types de personalisations

        \begin{itemize}
            \item Adaptation aux habitudes de la personne.
            % les rdv sont plutot le dimanche, le medecin s'appelle X, la personne habide dans un coin avec certains restaurant
            \item \textbf{Adaptation au type de personne.}
            % personne agée, enfant, dyslesique, culture (red good chine, red wrong europe) etc
        \end{itemize}

        %C'est surtout la deuxième problématique qui nous interesse.

        \begin{alertblock}{Problématique}
            Comment s'adapter sans a priori? Utiliser l'apprentissage par transfert.
            % mais tout d'abord qu'est ce qu'un système de dialogue
        \end{alertblock}

    \end{frame}


    \section{Systèmes de dialogue orientées tâches}



    \subsection{Une architecture modulaire}


    \begin{frame}{Le Slot-Filling}

        Le système doit remplir un formulaire (\textit{informable}) pour répondre à une réquête (\textit{requestable}).\\

        \begin{block}{Exemple: reservation de restaurants.}
        \begin{itemize}
            \item \textit{informable}: localisation, fourchette de prix ...
            \item \textit{requestable}: \textit{informable}+ l'adresse exacte du restaurant, le numéro de téléphone ...
        \end{itemize}
        \end{block}

    \end{frame}

    \foreach \n in {1,2,3,4,5,6,7}{
    \begin{frame}{Le processus}
        \begin{figure}
            \centering
            %%\vspace{-1.5em}
            %\todo{add the last slide with N2N}
            \includegraphics[scale=0.7,page=1]{img/pipeline\n}
        \end{figure}
    \end{frame}
    }

    \foreach \n in {1,2}{
    \begin{frame}{Un problème de décision séquentielle}
        \begin{figure}
            \centering
            %%\vspace{-1.5em}
            \includegraphics[scale=0.55,page=1]{img/pipeline-dp\n}
        \end{figure}
    \end{frame}
    }




    \begin{frame}{Un problème de décision séquentielle}
        %On réduit le problème à cette échange
        \begin{figure}
            \centering
            %%\vspace{-1.5em}
            \includegraphics[scale=1.0,page=1]{../sources/dm-rl/rl-pipeline}
        \end{figure}
    \end{frame}
    \begin{frame}
        \begin{block}{Succès du dialogue}
            \begin{itemize}
                \item Toutes les contraintes de l'utilisateur sont remplies
                \item Sans erreurs dans la communication
            \end{itemize}
        \end{block}

        \begin{block}{Actes de dialogues}
            \begin{itemize}
            \item \texttt{request}
            \item \texttt{inform}
            \item \texttt{repeat}
            \item \texttt{explicit/implicit-confirmation}
            \end{itemize}
        \end{block}

        \begin{block}{Etats}
            \begin{itemize}
                \item Toutes les actions du systèmes.
                \item Les scores de confiance de chaque slots
                \item Toutes les demandes de l'utilisateur.
            \end{itemize}
        \end{block}

        \begin{alertblock}{}
            Comment décider l'action à choisir en fonction de l'état courant ?
        \end{alertblock}
    \end{frame}

    \subsection{Apprentissage par renforcement}
    \begin{frame}{Markov Decision Processes}

        Un MDP est un tuple $\langle{}\cS,\cA,\reward,\transition,\discountfactor\rangle{}$ où:
        \begin{itemize}
            \item  $\cS$ est l'espace des états,
            \item  $\cA$ est l'espace des actions,
            \item $\reward\in\Real^{\cS \times \cA}$ est la fonction de récompense,
            \item $\transition\in \cM(\cS)^{\cS \times \cA}$ is the transition kernel; $\cM(\cX)$ denotes the probability measure over a set $\cX$.
            \item $\discountfactor$ is the discount factor.
        \end{itemize}

    \end{frame}

    \begin{frame}{Résoudre un MDP}
        \begin{itemize}
            \item A policy $\policy\in\cM(\cA)^{\cS}$ % that maps states to actions
            \item The return $\return^{\policy} = \sum_{\indextransition=0}^\infty \discountfactor^{\indextransition} \reward(s_{\indextransition},a_{\indextransition})$% expliquer s_i et a_i
            \item Optimal policy :  $\policy^* = \argmax\limits_{\policy} \mathbb{E}_{\policy} [\return^{\policy}|s_0=s,a_0=a]\ \forall (s,a) \in \cS\times\cA$

            \item $\Q^*(s,a)=\reward(s,a) +\discountfactor \sum_{s'\in \cS}[\transition(s,a,s')\max_{a'\in \cA}\Q^*(s',a')] = \bo Q^*.$
            % It exists a unique function, denoted as $\Q^*$, that verifies the Bellman Optimality equation:

            \item $\optimalpolicy(s) \in \argmax\limits_{a\in \cA} \Q^*(s,a)$ is optimal.
        \end{itemize}


    \end{frame}

    \begin{frame}{Algoritmes}

        \begin{block}{Value-Iteration}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q(s,a) \leftarrow \bo Q(s,a)\ \forall (s,a)$ until convergence
                \item Return $\pi(s) = \argmax\limits_{a} Q(s,a)$
            \end{itemize}
        \end{block}

        \begin{alertblock}{}
            \begin{itemize}
                \item How to compute $\bo Q$ if $\transition$ and $\reward$ unknown?
                \item How to compute $Q \forall (s,a) \in \cS\times\cA$ if $S$ is continous or very large ?
            \end{itemize}

        \end{alertblock}

        \begin{block}{Fitted-Q}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q \leftarrow \Gamma(\{s_{\indextransition},a_{\indextransition}\}_{{\indextransition}\in \T},\{r'_{\indextransition} + \discountfactor  \max_{a'\in\cA} \Q(s'_i,a')\}_{{\indextransition} \in \T})$
                \item Return $\pi(s) = \argmax\limits_{a} Q(s,a)$
            \end{itemize}
        \end{block}


    \end{frame}

    \begin{frame}{}


        \begin{alertblock}{}
            Comment apprendre face à un utilisateur inconnu?
        \end{alertblock}


        \begin{itemize}
            \item (-) Amasser des transitions et apprendre en ligne (\textit{from scratch})
            \item (+) Mise à profit de notre expérience avec d'autres utilisateurs
            \begin{itemize}
                \item Apprentissage par transfert
            \end{itemize}

        \end{itemize}


    \end{frame}

    \subsection{Apprentissage par Transfert}
    \begin{frame}{Pourquoi transférer ?}

        \begin{figure}
            \begin{center}
                \subfloat[jumpstart performance]{
                \includegraphics[width=0.33\textwidth]{../sources/dm-tl/objectives-jumpstart}
                \label{fig:objectives-jumpstart}
                }
                \subfloat[Learning performance]{
                \includegraphics[width=0.33\textwidth]{../sources/dm-tl/objectives-learn}
                \label{fig:objectives-learn}
                }
                \subfloat[Asymptotic performance]{
                \includegraphics[width=0.33\textwidth]{../sources/dm-tl/objectives-asymptote}
                \label{fig:objectives-asymptote}
                }
                \caption{Objectives of Transfert Learning~\parencite{langley2006,LazaricSurvey}}
                \label{fig:objectives}
            \end{center}
        \end{figure}

    \end{frame}

    \begin{frame}{Les différentes formes de transfert}


        \begin{table}
            \centering
            \resizebox{\textwidth}{!}{
            \begin{tabular}{llll}
                \hline
                & Knowledge & Setting & Metric \\ \hline

                \hline
                \textbf{cross domain} &&&\\
                \hline
                \cite{Gasic2013}& parameter (transitions/policy)  & \makecell[l]{mono-task} & all         \\
                \cite{Keizer2018-tl-dialogue}& parameter (policy)& \makecell[l]{generic-task} & all      \\
                \cite{Chen2018-tl-dialogue}& parameter (policy) & \makecell[l]{generic-task} & all       \\
                \cite{Ilievski2018-tl-dialogue}& parameter (policy) & \makecell[l]{mono-task}& all      \\
                \hline
                \textbf{user adaptation} &&&\\
                \hline
                \cite{casanueva2015-tl-dialogue}& parameter (transitions/policy) & \makecell[l]{multi-task} & all         \\
                \cite{Genevay2016}& \makecell[l]{transitions \\ parameter (policy)} & \makecell[l]{multi-task}& all   \\
                \cite{Mo2018-tl-dialogue-PETAL}& parameter ($\Q$-function, DST) & \makecell[l]{generic-task}& all        \\
                \hline
                \textbf{dialogue evaluation} &&&\\
                \hline
                \cite{elasri2014-tl-dialogue}& representation (reward function) & \makecell[l]{generic-task} & all        \\
                \hline

            \end{tabular}
            }
            \caption{Transfer Learning for Dialogue Systems}
            \label{table:survey}
        \end{table}


    \end{frame}

    \begin{frame}{Contributions}

        %On va explorer
        Deux directions, toutes deux exploitant l'apprentissage par transfert.

        \begin{itemize}
            \item Approche classique, avec l'utilisation d'outils existants et leur mise à l'echelle.
            \item Approche plus conservatrice, en mettant en avant la notion de risque dans la conversation.
        \end{itemize}


    \end{frame}

    \section{Passage à l'échelle de l'apprentissage par transfert}

    \subsection{Un processus complet pour l'adaptation à l'utilisateur}



    \foreach \n in {0,1,2,3,4,5,6,7,8,9,10,11,13,14}{
    \begin{frame}{Le processus}
        %{but what if the system database grow ? bandit useless}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.75\textwidth]{img/dataflowRobot\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{\textbf{Sources election} | \textsc{PD-distance}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1.0\textwidth]{img/pddistance0.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Sources election} | \textsc{PD-distance}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1.0\textwidth]{img/pddistance.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    \foreach \n in {0,1,2,3,4}{
    \begin{frame}{\textbf{Sources election} | \textsc{PD-distance} and euclidian norm}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1.0\textwidth]{img/euclide\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{\textbf{Sources election} | \textsc{K-means}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/clustering.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \foreach \n in {0}{
    \begin{frame}{\textbf{Sources election} | \textsc{K-means}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/kmeans\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }
    \begin{frame}{\textbf{Sources election} | \textsc{K-medoids}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/clustering.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Sources election} | \textsc{K-medoids}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/kmedoids.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Adaptation experiments} | simulated users}
        \begin{figure}
            \captionsetup[subfigure]{labelformat=empty}
            \begin{center}
                \subfloat[Overall score ]{
                \includegraphics[width=0.33\textwidth]{img/handcraftedScores.pdf}
                }
                \subfloat[Longueur du dialogue]{
                \includegraphics[width=0.33\textwidth]{img/handcraftedDialoguesize.pdf}
                }
                \subfloat[Task completion]{
                \includegraphics[width=0.33\textwidth]{img/handcraftedTC.pdf}
                }
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Adaptation experiments} | human-model users}
        \begin{figure}
            \captionsetup[subfigure]{labelformat=empty}
            \begin{center}
                \subfloat[Overall score ]{
                \includegraphics[width=0.33\textwidth]{img/humanScores.pdf}
                }
                \subfloat[Longueur du dialogue]{
                \includegraphics[width=0.33\textwidth]{img/humanDialoguesize.pdf}
                }

                \subfloat[Task completion]{
                \includegraphics[width=0.33\textwidth]{img/humanTC.pdf}
                }
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textbf{Cross comparison} | human-model users}
        \begin{figure}
            \begin{center}
                \begin{tabular}{|l|c|c|c|c|c|}
                    \hline
                    \backslashbox{u}{s} & vsAlex & vsNico & vsWill & vsMerwan  \\
                    \hline
                    Alex & \textbf{1,074}   & 1,021 & 1,073 & 1,071        \\
                    Nico & 1,249 &     \textbf{1,251}     & 1,249 & 1,240    \\
                    Will & 1,126 & 1,104 &   \textbf{1,127}      & 1,120    \\
                    Merwan & 0,989 & 0,858 & 0,980 &    \textbf{ 0,998} \\
                    \hline
                \end{tabular}
            \end{center}
        \end{figure}
    \end{frame}


    \begin{frame}{\textbf{Cross comparison} | simulated users}
        \begin{figure}
            \begin{center}
                \setlength{\tabcolsep}{0.15em}
                \begin{tabular}{|l|c|c|c|c||c|c|c|c|c|c|c|c|c|}
                    \hline
                    \backslashbox{u}{s}& type & $c_{\top}$ & $c_{\bot}$ & x&vspu1 & vspu2 & vspu3 & vspu4 & vspu5 & vspu6 & vspu7   \\
                    \hline

                    pu1 & DU & 1 & -1 & 0.1 &  \textbf{0,62 }& 0,44 & 0,46 & 0,40 & 0,40 &0,40 & 0,59    \\
                    pu2 & DU & 5 & -5 & 0.1 & 0,53 &  \textbf{0,82}  & 0,81& 0,51 & 0,70 & 0,41 & 0,71 \\
                    pu3 & DU & 5 & -5 & 0.2& 0,53 &  \textbf{0,81} & \textbf{0,81}  & 0,52 & 0,72 & 0,42 & 0,71  \\
                    pu4 & RU & 5 & -5 & 0.1& 0,42 & 0,94 & 0,94 & \textbf{1,00}& 0,92 & 0,85 & 0,94 \\
                    pu5 & ARPBU & 1 & -1& &0,84& 0,98 & 1,00 & 1,11 &\textbf{1,16}& 1,13 & 1,05    \\
                    pu6 & AAU & 1 & -1 & &0,95 & 1,06 & 1,07 & 1,29 & 1,27 &\textbf{1,30} & 1,06 \\
                    pu7 & SAOTU & 1 & -1& &0,43 & 0,26& 0,27 & 0,10 & 0,18 & 0,03 &\textbf{ 0,58}  \\
                    \hline
                \end{tabular}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}
        \begin{itemize}
            \item (+) la solution fonctionne (en HDC)
            \item (.) heavily engineered
            \begin{itemize}
                \item (+) chaque composant peut être testé et validé
                \item (-) chaque composant générère des approximations et erreurs.
                \item (-) formulation complexe
                \item (-) meta paramètres
            \end{itemize}
        \end{itemize}

        Et si on pouvait:
        \begin{itemize}
            \item (+) une solution scalable naturellement
            \item (+) embarqué directement dans un algorithm "low level"
            \item (+) avec une formulation simple.
            \item $\rightarrow$ Solution: Transfer Deep Q-Learning (TDQN)
        \end{itemize}

    \end{frame}


    \subsection{Transfert continu}

    \foreach \n in {0,1,2,3,4,5,6,7,8,9,10,11}{
    \begin{frame}{TDQN}

        %\todo{overlay "won't work"}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1\textwidth]{drawio/out/tdqn\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \subsection{}

    \begin{frame}
        Lors de l'adoption d'un nouveau système, l'utilisateur est plus suceptible d'abandonner rapidement.
        \begin{block}{}
            \begin{itemize}
                \item On va apprendre une nouvelle strategie de dialogue,
                \item tout en prenant soin de ne pas "brusquer" l'utilisateur
                \begin{itemize}\item notion de \textit{safety}\end{itemize}
            \end{itemize}
        \end{block}

    \end{frame}

    \section{Apprentissage par transfert sécurisé}

    \subsection{epsilon safe}

    \begin{frame}
        On rajoute une info supplementaire (safety): %lié intimement à la reception du signal de reward:
        \begin{itemize}
            \item (+) peut donc guider l'apprentissage (safe $\rightarrow$ reward)
            \item (+) d'une manière plus meta, eviter les dialogues trop catastrophiques pour eviter les dropouts.
        \end{itemize}

    \end{frame}

    \begin{frame}

        \begin{figure}
            \begin{center}
                \includegraphics[width=0.5\textwidth]{img/transfer7.pdf}
            \end{center}
        \end{figure}

    \end{frame}

    \begin{frame}{Expériences}

        \begin{block}{Environnement}
            \begin{itemize}
                \item \texttt{ask-next}%: ask next slot (with ASR errors).
                \item \texttt{repeat-oral} (safe + ASR errors)%: repeat current slot (with ASR errors).
                \item \texttt{repeat-numpad} (unsafe)%: repeat current slot using numeric pad (without ASR errors).
                \item \texttt{summarize-inform}
                %: summarise slots values and return the form result. If values are correct, the dialogue ends successfully, if not, the slot values are reset and the dialogue continues from the first slot.
            \end{itemize}

            Raccrochage: $C: \cS\times\cA \rightarrow \{0,1\}$

        \end{block}

        \begin{block}{Apprentissage}
            \begin{itemize}
                \item FTQ avec $R \leftarrow R + \lambda C$
                \item Safe: $\lambda = 1000$
                \item Unsafe: $\lambda = 0$ (baseline)
            \end{itemize}
        \end{block}


    \end{frame}

    \begin{frame}{Expériences}

        \begin{figure}
            \captionsetup[subfigure]{labelformat=empty}
            \begin{center}
                \subfloat[Score du dialogue]{
                \includegraphics[width=0.5\textwidth]{img/retgreed.png}
                }
                \subfloat[Dialogue arrive à terme]{
                \includegraphics[width=0.5\textwidth]{img/rewgreed.png}
                }
            \end{center}
        \end{figure}


    \end{frame}
    %    \begin{frame}{Expériences}
    %
    %        \begin{block}{Resultats}
    %            \begin{itemize}
    %                \item (+) Dialogues très légérement plus sûr. % on garde donc l'user
    %                \item (-) mais cela n'affecte pas le score
    %                % et donc La vitesse d'apprentissage.
    %            \end{itemize}
    %        \end{block}
    %
    %    \end{frame}
    \begin{frame}

        On a utilisé une politique lagragienne pour généré des dialogue safe.

        \begin{alertblock}{Limitation}
            La formulation linéaire de la reward implique des comportements extreme de $\pi_{safe}$: trop safe/trop risky.
            % , Il est donc possible que les comportements soient les mêmes avec ou sans signal de safety. Donc ça revient a avoir deux politiques greedy (ou lieu d'une safe+greedy)
        \end{alertblock}

        \begin{block}{Solution: calibration de lambda}% oral: limitation des politiques lagragiennes
            On cherche notre strategie safe sur le front de pareto
        \end{block}


    \end{frame}

    \foreach \n in {0,1}{
    \begin{frame}{}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1\textwidth]{drawio/out/pareto\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \begin{frame}
        \begin{alertblock}{Limitations}
            \begin{itemize}
                \item (-) processus lourd et approximatif % où commencer, quel steps ?
                \item (-) il se peut qu'il n'existe même pas de lambda
                % pas de formulation linéaire de la reward pour un budget donné
            \end{itemize}
        \end{alertblock}
        \begin{exampleblock}{Solution}
            Budgeted Reinforcement Learning.
            % Ce qui nous amène à la prochaine contribution: BRL
        \end{exampleblock}
    \end{frame}

    \subsection{Maîtrise du risque en RL}

    \begin{frame}{Une formulation plus directe}

        \begin{itemize}
            \item Amasser des échanges sans perdre l'utilisateur% Ce qui nous interesse en réalité, c'est d'amasser des données personalisées sans perdre l'utilisateur.
            \item Utilisation d'une politique sécurisée % On propose d'utiliser une politique safe généralisée dès les premiers dialogues.
            \item Formulation linéaire de la reward insuffisante
            % Peut t on se contenter d'une politique lagragienne ? Non
        \end{itemize}


    \end{frame}

    \begin{frame}{Setting}
        \begin{block}{Markov Decision Process}
            \begin{itemize}
                \pause\item $(\cS, \cA, P, R_r, \gamma)$
                \pause\item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R_r(s_t, a_t)$ the $\gamma$-discounted return of rewards.
                \pause\item Find $\pi^*$ s.t $\forall s\in\cS$:
                \begin{equation}
                    \label{eq:mdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA)^\cS} \expectedvalue[G_r^\pi | s_0=s]
                    \end{array}
                \end{equation}

            \end{itemize}
        \end{block}


        \begin{block}{}
            \begin{itemize}
                \item \colorbox{green}{(+) \textit{Tractable}}
                \item \colorbox{red}{(-) Calibration nécessaire (quel $\lambda$ ?)}
                % A l ORAL si formulation sous contrainte
            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Setting}

        \begin{block}{\textcolor{blue}{Constrained} Markov Decision Process}
            \begin{itemize}
                \item $(\cS, \cA, P, R_r,\textcolor{blue}{R_c}, \gamma,\textcolor{blue}{\beta})$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R_r(s_t, a_t)$ the $\gamma$-discounted return of rewards.
                \item \textcolor{blue}{ $G_c^\pi = \sum_{t=0}^\infty \gamma^t R_c(s_t, a_t)$ the $\gamma$-discounted return of costs.}
                \item Find $\pi^*$ s.t $\forall s\in\cS$:
                \begin{equation}
                    \label{eq:cmdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA)^\cS} \expectedvalue[G_r^\pi | s_0=s] \\
                        \text{ s.t. }  \textcolor{blue}{\expectedvalue[G_c^\pi | s_0=s] \leq \beta}
                    \end{array}
                \end{equation}
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item \colorbox{green}{(+) \textit{Tractable}}
                \item \colorbox{green}{(+) DOF supplémentaire}% A L ORAL On peut définir un budget de safety , formuation plus naturelle
                \item \colorbox{red}{(-) Calibration nécessaire (budget fixé)}
                % A L ORAL si le budget change on the fly, ou qu'il n'est pas adapté, on doit reapprendre une politique,  Or Le front de pareto est rarement linéaire, le choix du budget n'est pas évident


            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Setting}

        \begin{block}{\textcolor{blue}{Budgeted} Markov Decision Process}
            \begin{itemize}
                \item $(\cS, \cA, P, R_r,{R_c}, \gamma,\textcolor{blue}{\cB})$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R_r(s_t, a_t)$ the $\gamma$-discounted return of rewards.
                \item  $G_c^\pi = \sum_{t=0}^\infty \gamma^t R_c(s_t, a_t)$ the $\gamma$-discounted return of costs.
                \item Find $\pi^*$ s.t $\forall (s,\textcolor{blue}{\beta})\in\cS\times\textcolor{blue}{\cB}$:
                \begin{equation}
                    \label{eq:bmdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA\times\textcolor{blue}{\cB})^{\cS\times\textcolor{blue}{\cB}}} \expectedvalue[G_r^\pi | s_0=s,\textcolor{blue}{\beta_0=\beta}] \\
                        \text{ s.t. }  \expectedvalue[G_c^\pi | s_0=s,\textcolor{blue}{\beta_0=\beta}] \leq \beta
                    \end{array}
                \end{equation}
            \end{itemize}
        \end{block}


        \begin{block}{}
            \begin{itemize}
                \item \colorbox{red}{(-) \textit{Non Tractable}}
                \item \colorbox{green}{(+) Pas besoin de calibration, le budget est un paramètre de la politique}
                \item \colorbox{green}{(+) DOF supplémentaire}
                % A L ORAL On peut définir un budget de safety et d'afranchir des lambda.}
            \end{itemize}
        \end{block}

    \end{frame}


    \begin{frame}{Augmented Settings}

        \textbf{Budgeted policies} $\pi$
        \begin{itemize}
            \pause\item $ \pi:\underbrace{(s,\beta)}_{\os} \rightarrow \underbrace{(a,\beta')}_{\oa}$
        \end{itemize}

        \textbf{Domain}
        \begin{itemize}
            \pause\item States $\ocS = \cS\times\cB$.
            \pause\item Actions $\ocA = \cA\times\cB$.
            \pause\item Dynamics $\ov{P}$
            %$\left((s',\beta') \condbar (s,\beta), (a, \beta_a)\right) \eqdef P(s'|s, a)\delta(\beta' - \beta_a)$.
        \end{itemize}
        \textbf{2D signals}
        \begin{enumerate}
            \pause\item Rewards $R = (R_r, R_c)$
            \pause\item Returns $G^\pi = (G_r^\pi, G_c^\pi)$
            \pause\item $V^\pi(\os) = (V_r^\pi, V_c^\pi) \eqdef \expectedvalue\left[ G^\pi \condbar \ov{s_0} = \os\right]$
            \pause\item $Q^\pi(\os, \oa)= (Q_r^\pi, Q_c^\pi) \eqdef \expectedvalue\left[ G^\pi \condbar \ov{s_0} = \os, \ov{a_0} = \oa\right]$
        \end{enumerate}

    \end{frame}

    \begin{frame}{Augmented Optimality}
        \begin{definition}
            In that order, we want to:
            \begin{enumerate}
                \item[(i)] \pause\colorbox{red}{Respect the budget $\beta$}:
                \begin{equation*}
                    \Pi_a(\os) \eqdef \{\pi\in\Pi: V_c^\pi(s, \beta) \mathcolorbox{red}{\leq \beta}\}
                \end{equation*}
                \item[(ii)] \pause\colorbox{green}{Maximise the rewards}:
                \begin{equation*}
                    V_r^*(\os) \eqdef \mathcolorbox{green}{\max}_{\pi\in\Pi_a(\os)}  V_r^\pi(\os) \qquad\quad \Pi_r(\os) \eqdef \mathcolorbox{green}{\argmax}_{\pi\in\Pi_a(\os)}  V_r^\pi(\os)
                \end{equation*}
                \item[(iii)] \pause\colorbox{yellow}{Minimise the costs}:
                \begin{equation*}
                    V_c^*(\os) \eqdef \mathcolorbox{yellow}{\min}_{\pi\in\Pi_r(\os)}  V_c^\pi(\os), \qquad\quad \Pi^*(\os) \eqdef \mathcolorbox{yellow}{\argmin}_{\pi\in\Pi_r(\os)}  V_c^\pi(\os)
                \end{equation*}
            \end{enumerate}

            \pause We define the budgeted action-value function $Q^*$ similarly
        \end{definition}
    \end{frame}

    \begin{frame}{Budgeted Bellman Optimality Equation}
        \begin{theorem}[Budgeted Bellman Optimality Equation]
            $Q^*$ verifies the following equation:
            \begin{align*}
                Q^{*}(\os, \oa) &= \cT Q^{*}(\os, \oa)\\
                &\eqdef R(\os, \oa) + \gamma \sum_{\os'\in\ocS} \ov{P}(\ov{s'} | \os, \oa)\sum_{\ov{a'}\in \ocA} \pi_\text{greedy}(\ov{a'}|\ov{s'}; Q^*) Q^{*}(\ov{s'}, \ov{a'}),
            \end{align*}
            where the greedy policy $\pi_\text{greedy}$ is defined by:
            \begin{align*}
                \pi_\text{greedy}(\cdot|\os; Q) \in &\mathcolorbox{yellow}{\argmin}_{\rho\in\Pi_r^Q} \expectedvalueover{\oa\sim\rho}Q_c(\os, \oa), \\
                \text{where }\quad \Pi_r^Q \eqdef &\mathcolorbox{green}{\argmax}_{\rho\in\cM(\ocA)} \expectedvalueover{\oa\sim\rho} Q_r(\os, \oa) \\
                & \text{ s.t. }  \expectedvalueover{\oa\sim\rho} Q_c(\os, \oa) \mathcolorbox{red}{\leq \beta}
            \end{align*}
        \end{theorem}
        % ORAL: comment résoudre ce problème ?
    \end{frame}


    \foreach \n in {0,1,2,3,4}{
    \begin{frame}{Résoudre le problème non linéaire}
        \begin{figure}
            \centering
            %%\vspace{-1.5em}
            \includegraphics[scale=1.0,page=1]{img/b\n}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{Comportement asymptotique}

        \begin{itemize}
            \item \colorbox{red}{$\cT$ n'est pas une contraction}
            \item \colorbox{green}{sauf quand les $Q$-fonctions sont lisses ie:}
            \begin{itemize}
                \item "$Q_r$ est $L$-Lipschitz par rapport à $Q_c$", avec $L<\frac{1}{\gamma}-1$
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}

        \begin{block}{Budgeted Value-Iteration}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q(\os,\oa) \leftarrow \bo Q(\os,\oa)\ \forall (\os,\oa)$ until convergence (which might not happen)
                \item Return $\pi(\os) = \pi_\text{greedy}(\cdot|\os; Q)$
            \end{itemize}
        \end{block}

        \begin{alertblock}{}
            \begin{itemize}
                \item How to compute $\bo Q$ if $\transition$, $\reward$ and $\constraint$ are unknown?
                \item How to compute $Q \forall (\os,\oa) \in \ocS\times\ocA$ if $\ocS$ is continous or very large ?
            \end{itemize}

        \end{alertblock}

        \begin{block}{Budgeted Fitted-Q}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q \leftarrow \Gamma(\{\os_{\indextransition},\oa_{\indextransition}\}_{{\indextransition}\in \T},\{r'_{\indextransition} + \gamma \sum_{\ov{a}'\in \cA} \pi_\text{greedy}(\ov{a}'|\ov{s}'_{\indextransition}; Q) Q(\ov{s}'_{\indextransition}, \ov{a}')_{{\indextransition} \in \T})$
                \item Return $\pi(\os) = \pi_\text{greedy}(\cdot|\os; Q)$
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Experiments}
        \begin{center}
            \includegraphics[width=0.75\textwidth]{img/slot-filling.pdf}
        \end{center}
    \end{frame}

    \begin{frame}{Risk-sensitive exploration}

        How to collect the batch $\cD$?

        \begin{itemize}
            \item We propose an $\epsilon$-greedy exploration procedure
            \begin{itemize}
                \pause\item Sample an initial budget $\beta_0$
                \pause\item At each step, where $\os=(s,\beta)$ only explore feasible budgets:
                \pause\begin{align*}
                          &\oa = (a, \beta_a)\sim\mathcal{U}(\Delta_{\cA\cB})\\
                          &\text{ where }  \Delta \text{ is such that }\probability{a, \beta_a|s, \beta} \text{verifies} \expectedvalue[\beta_a]\leq\beta
                \end{align*}
            \end{itemize}
        \end{itemize}


    \end{frame}

    \begin{frame}{Experiments: 2D world}
        \begin{center}
            \includegraphics[page=1, width=0.75\textwidth]{img/corridors}
        \end{center}
    \end{frame}

    \section{}
    \subsection{}
    \begin{frame}{Conclusion}
        \begin{itemize}
            \item Prise de décision dans un SDS = le gestionnaire de dialogue
            \item Formulation avec du RL
            \item Comment s'adapater rapidement à un nouvel utilisateur ?
            \begin{itemize}
                \item Apprentissage par transfert
            \end{itemize}
            \item Deux directions:
            \begin{itemize}
                \item Mise à l'échelle de solutions existantes
                \begin{itemize}
                    \item PD-distance: Clustering de politiques
                    \item TDQN: Solution de transfert en continu avec DQN
                \end{itemize}
                \item Approche sécurisée:
                \begin{itemize}
                    \item $\epsilon$-safe, formulation lagragienne insuffisante
                    \item $BRL$
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{frame}


    %    \begin{frame}
    %        \printbibliography[heading=bibempty]
    %
    %    \end{frame}

\end{document}

