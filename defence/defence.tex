\documentclass[french]{beamer}
\usetheme[secheader]{Boadilla}
\usecolortheme[named=blue]{structure}
\usepackage[utf8]{inputenc}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}

\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{tabularx}
\usepackage{makecell}


\usefonttheme{serif}
\usepackage{xkeyval}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[acronym,toc,symbols]{glossaries}
\usepackage{subfig}
\usepackage[style=authoryear-icomp,maxbibnames=9,maxcitenames=2,uniquelist=false,
backend=biber]{biblatex}
\usepackage{diagbox}
\usepackage{graphicx}
%\usepackage[block=space,style=authoryear,citestyle=authoryear,sorting=nyt,sortcites=true,autopunct=true,autolang=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
%\usepackage[backend=biber]{biblatex}
\definecolor{macouleur}{rgb}{0.75,0.43,0.09}
%\AtBeginSection[]{
%{
%%\setbeamercolor{background canvas}{bg=macouleur}
%\begin{frame}
%    \vfill
%    \centering
%    \begin{beamercolorbox}[sep=15pt,center,shadow=true,rounded=true]{title}
%        %\usebeamerfont{title}\insertsectionhead\par
%        %\frametitle{Outline for section \thesection}
%        \tableofcontents[currentsection]
%    \end{beamercolorbox}
%    \vfill
%\end{frame}
%}
%}


%\AtBeginSubsection[]{
%{
%\begin{frame}
%    \vfill
%    \centering
%    \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
%        \usebeamerfont{title}\insertsubsectionhead\par
%        %
%    \end{beamercolorbox}
%    \vfill
%\end{frame}
%}
%}

\addbibresource{../bibliography.bib} % BibTeX bibliography file
\defbibheading{bibempty}{}

\input{../mathdef.tex}
%\input{../glossary.tex}
\input{symbol.tex}

\presetkeys{todonotes}{inline}{}
\title[TITLE]{Apprentissage par renforcement pour l'optimisation des systèmes de dialogue via l'adaptation à l'utilisateur.}
\subtitle{Soutenance de thèse.}
\author{Nicolas Carrara}
\institute[ULille]{Université de Lille}
\date{Le 18 Décembre 2019}

\setbeamertemplate{footline}{%
\hfill%
\usebeamercolor[fg]{page number in head/foot}%
\usebeamerfont{page number in head/foot}%
\insertframenumber%
%\,/\,\inserttotalframenumber
\kern1em\vskip2pt%
}

\beamertemplatenavigationsymbolsempty

\newcommand{\mypm}{\mathbin{\mathpalette\@mypm\relax}}

\newcommand{\cplus}{\colorbox{green}{($+$)} }
\newcommand{\cmoins}{\colorbox{red}{($-$)} }
\newcommand{\cmean}{\colorbox{yellow}{($\pm$)}}

\begin{document}

    \begin{frame}
        \maketitle
        \centering
        \resizebox{0.65\textwidth}{!}{
        \begin{tabular}{l l l}

            Pr. Philippe \textbf{Preux} & Université de Lille & Président du jury \\
            Dr. Alain \textbf{Dutech} & INRIA & Rapporteur \\
            Pr. Fabrice \textbf{Lefèvre} & Université d'Avignon & Rapporteur \\
            Dr. Lina Maria \textbf{Rojas-Barahona}  & Orange Labs & Examinatrice \\
            Pr. Olivier \textbf{Pietquin} & Université de Lille, Google Research & Directeur de thèse \\
            Dr. Romain \textbf{Laroche} & Microsoft Research & Encadrant industriel \\
            Dr. Tanguy \textbf{Urvoy}& Orange Labs & Encadrant industriel \\
        \end{tabular}
        }
        \includegraphics[width=0.7\textwidth]{img/logos}
    \end{frame}


    \section{Introduction}


    \begin{frame}{Trois types de systèmes de dialogue}
        \begin{columns}
            \begin{column}{0.5\textwidth}
                \begin{itemize}
                    \item Discussion social sans but précis.
                    \item Question-Réponse
                    \item \textbf{Dialogues orientés tâches:}
                    \begin{itemize}
                        \item Reservation de restaurant,
                        \item virement bancaire
                        \item etc
                    \end{itemize}
                \end{itemize}
            \end{column}
            \begin{column}{0.5\textwidth}  %%<--- here
                \begin{center}
                    \includegraphics[width=1.0\textwidth]{drawio/out/sds.pdf}
                \end{center}
            \end{column}
        \end{columns}
    \end{frame}

    \begin{frame}{Problématique}
        \begin{columns}
            \begin{column}{0.5\textwidth}
                Besoin de systèmes personalisés.
                \begin{itemize}
                    \item Adaptation aux données de la personne.
                    %  le medecin s'appelle X, la personne habide dans un coin avec certains restaurant
                    \item \textbf{Adaptation au type/habitude de la personne.}
                    % personne agée, enfant, dyslesique, culture (red good chine, red wrong europe) etc
                \end{itemize}
            \end{column}
            \begin{column}{0.5\textwidth}  %%<--- here
                \begin{center}
                    \includegraphics[width=1.0\textwidth]{img/adap1.pdf}
                \end{center}
            \end{column}
        \end{columns}

        %C'est surtout la deuxième problématique qui nous interesse.
        \pause
        \begin{alertblock}{Problématique}
            Comment s'adapter rapidement? Utiliser l'apprentissage par transfert.
            % mais tout d'abord qu'est ce qu'un système de dialogue
        \end{alertblock}

    \end{frame}


    \section{Systèmes de dialogue en détail}



    \subsection{Une architecture modulaire}

    \begin{frame}{Le processus}
        \begin{block}{}
            \invisible{N/A}
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline-2}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            \textit{"Je voudrais réserver un restaurant chinois."}
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline-1}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            Je voulais réserver restaurant chinois $\rightarrow 0.75$
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline0}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            food-type="chinois"
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline1}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            food-type="chinois", 0.75, 0.97
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline2}
        \end{figure}
    \end{frame}



    \begin{frame}{Le processus}
        \begin{block}{  }
            \invisible{N/A}
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline3}
        \end{figure}
    \end{frame}


    \begin{frame}{Le processus}
        \begin{block}{  }
            food="chinois", 0.75, 0.97 | venue=None | price=None
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline4}
        \end{figure}
    \end{frame}


    \begin{frame}{Le processus}
        \begin{block}{}
            expl-conf=(type-food,"chinois")
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline5}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            Je comprend que vous souhaitez manger chinois, est-ce exact?
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline6}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            \textit{"Je comprend que vous souhaitez manger chinois, est-ce exact?"}
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline7}
        \end{figure}
    \end{frame}

    \begin{frame}{Le processus}
        \begin{block}{}
            \invisible{N/A}
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline8}
        \end{figure}
    \end{frame}

    \foreach \n in {9,10}{

    \begin{frame}{Prise de Décision Séquentielle}
        \begin{block}{  }
            \invisible{N/A}
        \end{block}
        \begin{figure}
            \centering
            \includegraphics[scale=0.55,page=1]{drawio/out/pipeline\n}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{Prise de Décision Séquentielle}


        \begin{columns}
            \begin{column}{0.5\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[scale=0.5,page=1]{drawio/out/pipeline12}
                \end{figure}
            \end{column}
            \pause
            \begin{column}{0.5\textwidth}  %%<--- here
                \begin{block}{Succès du dialogue ($r$)}
                    Si contraintes de l'utilisateur sont remplies, sans erreurs.
                \end{block}

                \begin{block}{Actes de dialogues ($a$)}
                    Request, repeat, explicit/implicit-confirm ...
                \end{block}

                \begin{block}{Etats ($s$)}
                    \begin{itemize}
                        \item Actions du système.
                        \item Scores de confiance.
                        \item Actions de l'utilisateur.
                    \end{itemize}
                \end{block}
            \end{column}
        \end{columns}
        \pause
        \begin{alertblock}{}
            Comment décider l'action en fonction de l'état courant ?
        \end{alertblock}

    \end{frame}



    \subsection{Apprentissage par renforcement}
    \begin{frame}{Processus de Decision Markovien}

        Un MDP est un tuple $\langle{}\cS,\cA,\reward,\transition,\discountfactor\rangle{}$ où:
        \begin{itemize}
            \item  $\cS$ est l'espace des états,
            \item  $\cA$ est l'espace des actions,
            \item $\reward: \cS\times\cA \rightarrow \Real$ est la fonction de récompense,
            \item $\transition: \cS\times\cA \rightarrow \cM(\cS)$ est la fonction de transition,
            \item $\discountfactor$ est le facteur d'actualisation.
        \end{itemize}

    \end{frame}

    \begin{frame}{Résoudre un MDP}

        \begin{block}{Définitions}
            \begin{itemize}
                \item  $\policy\in\cM(\cA)^{\cS}$ % that maps states to actions
                \item  $\return^{\policy} = \sum_{\indextransition=0}^\infty \discountfactor^{\indextransition} \reward(s_{\indextransition},a_{\indextransition})$
                % expliquer s_i et a_i

            \end{itemize}
        \end{block}
        \pause
        \begin{block}{Optimalité}
            \begin{itemize}
                \pause\item $\policy^* = \argmax\limits_{\policy} \mathbb{E}_{\policy} [\return^{\policy}|s_0=s,a_0=a]\ \forall (s,a) \in \cS\times\cA$
                \pause\item $\Q^*(s,a)=\reward(s,a) +\discountfactor \sum_{s'\in \cS}[\transition(s,a,s')\max_{a'\in \cA}\Q^*(s',a')] = \bo Q^*.$
                % It exists a unique function, denoted as $\Q^*$, that verifies the Bellman Optimality equation:

                \pause\item $\optimalpolicy(s) \in \argmax\limits_{a\in \cA} \Q^*(s,a)$ est optimale.
            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Algoritmes}

        \begin{block}{Value-Iteration}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q(s,a) \leftarrow \bo Q(s,a)\ \forall (s,a)$ jusqu'à convergence
                \item Renvoyer $\pi(s) = \argmax\limits_{a} Q(s,a)$
            \end{itemize}
        \end{block}
        \pause
        \begin{alertblock}{}
            \begin{itemize}
                \item Comment calculer $\bo Q$ si $\transition$ et $\reward$ sont inconnues?
                \begin{itemize}
                    \item Estimateur avec $\mathcal{D}=\{(s_i,a_i,r_i,s_i')\}_{i\in[0,N]}$
                \end{itemize}
                \item Comment calculer $Q\ \forall (s,a) \in \cS\times\cA$ si $S$ est très grand voir continu ?
                \begin{itemize}
                    \item Approximation de fonction
                \end{itemize}
            \end{itemize}

        \end{alertblock}
        \pause
        \begin{block}{Fitted-Q}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q \leftarrow \Gamma(\{s_{\indextransition},a_{\indextransition}\}_{{\indextransition}\in \T},\{r_{\indextransition} + \discountfactor  \max_{a'\in\cA} \Q(s'_i,a')\}_{{\indextransition} \in \T})$ jusqu'à convergence
                \item Retourner $\pi(s) = \argmax\limits_{a} Q(s,a)$
            \end{itemize}
        \end{block}


    \end{frame}

    \begin{frame}{}


        \begin{alertblock}{}
            Comment apprendre face à un utilisateur inconnu?
        \end{alertblock}


        \begin{itemize}
            \item \cmoins Amasser des transitions et apprendre en ligne (\textit{from scratch})
            \item \cplus Mise à profit de notre expérience avec d'autres utilisateurs
            \begin{itemize}
                \item Apprentissage par transfert
            \end{itemize}

        \end{itemize}


    \end{frame}

    \subsection{Apprentissage par Transfert}
    \begin{frame}{Pourquoi transférer ?}

        \begin{figure}
            \begin{center}
                \subfloat[Performance au démarrage]{
                \includegraphics[width=0.33\textwidth]{../sources/dm-tl/objectives-jumpstart}
                \label{fig:objectives-jumpstart}
                }
                \subfloat[Performance d'apprentissage]{
                \includegraphics[width=0.33\textwidth]{../sources/dm-tl/objectives-learn}
                \label{fig:objectives-learn}
                }
                \subfloat[Performance asymptotique]{
                \includegraphics[width=0.33\textwidth]{../sources/dm-tl/objectives-asymptote}
                \label{fig:objectives-asymptote}
                }
                \caption{Les objectifs de l'apprentissage par transfert~(Langley 2016)}
                \label{fig:objectives}
            \end{center}
        \end{figure}

    \end{frame}

    \begin{frame}{Les différentes formes de transfert}
        \begin{itemize}
            \item Utiliser une ou des tâches sources,
            \item pour apprendre une ou des tâches cibles.
        \end{itemize}


        \begin{table}
            \centering
            \resizebox{0.7\textwidth}{!}{
            \begin{tabular}{lll}
                \hline
                & Connaissance & Cadre \\ \hline
                \textbf{cross domain} &&\\
                \hline
                \cite{Gasic2013}&transitions/politique & \makecell[l]{mono} \\
                \cite{Keizer2018-tl-dialogue}& politique& \makecell[l]{generique} \\
                \cite{Chen2018-tl-dialogue}& politique & \makecell[l]{generique}\\
                \cite{Ilievski2018-tl-dialogue}& politique & \makecell[l]{mono}\\
                \hline
                \textbf{user adaptation} &&\\
                \hline
                \cite{casanueva2015-tl-dialogue}& transitions/politique & \makecell[l]{multi}\\
                \cite{Genevay2016}&\makecell[l]{transitions, politique} &\makecell[l]{multi}\\
                \cite{Mo2018-tl-dialogue-PETAL}&$\Q$-fonction, DST &\makecell[l]{generique}\\
                \hline
            \end{tabular}
            }
        \end{table}


    \end{frame}

    \begin{frame}{Contributions}
        %On va explorer
        Deux directions, toutes deux exploitant l'apprentissage par transfert.
        \pause
        \begin{itemize}
            \item Utilisation d'outils existants et leur mise à l'echelle.
            \begin{itemize}
                \item Processus complet pour l'adaptation ~\parencite{ncarrara-online}
                \item Transfer Deep-$Q$-Network
            \end{itemize}
            \pause
            \item Approche sécurisée, notion de risque dans la conversation.
            \begin{itemize}
                \item Budgeted Reinforcement Learning~\parencite{ncarrara-brl}
                \item $\epsilon$-safe~\parencite{ncarrara-safe}
            \end{itemize}
        \end{itemize}


    \end{frame}

    \section{Passage à l'échelle de l'apprentissage par transfert}

    \subsection{Un processus complet pour l'adaptation à l'utilisateur}



    \foreach \n in {0,1,2,3,4,5,6,7,8,9,10,11,13,14}{
    \begin{frame}{Le processus}
        %{but what if the system database grow ? bandit useless}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.75\textwidth]{img/dataflowRobot\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{\textsc{PD-distance}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1.0\textwidth]{img/pddistance0.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textsc{PD-distance}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1.0\textwidth]{img/pddistance.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    \foreach \n in {0,1,2,3,4}{
    \begin{frame}{\textsc{PD-distance} et norme euclidienne}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1.0\textwidth]{img/euclide\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{\textsc{K-means}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/clustering.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \foreach \n in {0}{
    \begin{frame}{\textsc{K-means}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/kmeans\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }
    \begin{frame}{\textsc{K-medoids}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/clustering.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{\textsc{K-medoids}}
        \begin{figure}
            \begin{center}
                \includegraphics[width=0.85\textwidth]{img/kmedoids.pdf}
            \end{center}
        \end{figure}
    \end{frame}

    \begin{frame}{Expériences}
        \begin{block}{Environnement}
            The negotiation dialogue game~\parencite{Laroche2016}.
        \end{block}

        \begin{figure}
            \begin{center}
                \setlength{\tabcolsep}{0.15em}
                \begin{tabular}{|l|c|c|c|c||c|c|c|c|c|c|c|c|c|}
                    \hline
                    \backslashbox{u}{s}& type & $c_{\top}$ & $c_{\bot}$ & x&vspu1 & vspu2 & vspu3 & vspu4 & vspu5 & vspu6 & vspu7   \\
                    \hline

                    pu1 & DU & 1 & -1 & 0.1 &  \textbf{0,62 }& 0,44 & 0,46 & 0,40 & 0,40 &0,40 & 0,59    \\
                    pu2 & DU & 5 & -5 & 0.1 & 0,53 &  \textbf{0,82}  & 0,81& 0,51 & 0,70 & 0,41 & 0,71 \\
                    pu3 & DU & 5 & -5 & 0.2& 0,53 &  \textbf{0,81} & \textbf{0,81}  & 0,52 & 0,72 & 0,42 & 0,71  \\
                    pu4 & RU & 5 & -5 & 0.1& 0,42 & 0,94 & 0,94 & \textbf{1,00}& 0,92 & 0,85 & 0,94 \\
                    pu5 & ARPBU & 1 & -1& &0,84& 0,98 & 1,00 & 1,11 &\textbf{1,16}& 1,13 & 1,05    \\
                    pu6 & AAU & 1 & -1 & &0,95 & 1,06 & 1,07 & 1,29 & 1,27 &\textbf{1,30} & 1,06 \\
                    pu7 & SAOTU & 1 & -1& &0,43 & 0,26& 0,27 & 0,10 & 0,18 & 0,03 &\textbf{ 0,58}  \\
                    \hline
                \end{tabular}
            \end{center}
            \caption{Comparaison croisée}
        \end{figure}
    \end{frame}

    \begin{frame}{Expériences}


        \begin{figure}
            \captionsetup[subfigure]{labelformat=empty}
            \begin{center}
                \subfloat[Score]{
                \includegraphics[width=0.33\textwidth]{img/handcraftedScores.pdf}
                }
                \subfloat[Longueur du dialogue]{
                \includegraphics[width=0.33\textwidth]{img/handcraftedDialoguesize.pdf}
                }
                \subfloat[Réussite de la tâche]{
                \includegraphics[width=0.33\textwidth]{img/handcraftedTC.pdf}
                }
            \end{center}
            \caption{Adaptation}
        \end{figure}
    \end{frame}

    \begin{frame}
        \begin{itemize}
            \item \cplus La solution fonctionne,
            \item \cmean mais beaucoup d'ingénierie:
            \begin{itemize}
                \item \cplus chaque composant peut être testé et validé,
                \item \cmoins chaque composant générère des approximations et erreurs,
                \item \cmoins effort de développement important,
                \item \cmoins meta paramètres.
            \end{itemize}
        \end{itemize}
        \pause
        Et si on pouvait
        \begin{itemize}
            \item \cplus avoir une solution \textit{scalable} naturellement,
            \item \cplus embarquée directement dans un algorithme \textit{low-level}.
        \end{itemize}
        \begin{exampleblock}{Solution}
            Transfer Deep Q-Learning (TDQN)
        \end{exampleblock}
        $\rightarrow$ Pas abordé dans cette présentation.
    \end{frame}



    \section{Apprentissage par transfert sécurisé}


    \begin{frame}

        \begin{alertblock}{Limitation}
            Nouvel utilisateur $\rightarrow$ risque d'abandon élevé.
        \end{alertblock}


        \begin{exampleblock}{Solution}
            \begin{itemize}
                \item Apprentissage de la politique de dialogue,
                \item sans "brusquer" l'utilisateur,
                \item en utilisant une politique sécurisée.
                \begin{itemize}
                    \item Notion de budget $\beta$: maîtrise du risque de raccrochage.
                \end{itemize}
            \end{itemize}
        \end{exampleblock}

    \end{frame}



    \subsection{Maîtrise du risque en RL}

    \begin{frame}

        \begin{block}{Raccrochage}
            $C:S\times A \rightarrow \{0,1\}$.
        \end{block}

        \begin{block}{Relaxation lagrangienne}
            $R \leftarrow R - \lambda C$.
        \end{block}


        \begin{alertblock}{Limitation}
            Quel $\lambda$ pour quel budget $\beta$?
        \end{alertblock}

        \begin{exampleblock}{Solution: calibration de $\lambda$}% oral: limitation des politiques lagragiennes
            On cherche notre politique sécurisée sur le front de pareto
        \end{exampleblock}


    \end{frame}

    \foreach \n in {0,1}{
    \begin{frame}{}
        \begin{figure}
            \begin{center}
                \includegraphics[width=1\textwidth]{drawio/out/pareto\n.pdf}
            \end{center}
        \end{figure}
    \end{frame}
    }

    \begin{frame}
        \begin{alertblock}{Limitations}
            \begin{itemize}
                \item \cmoins Processus lourd et approximatif % où commencer, quel steps ?
                \item \cmoins Il se peut qu'il n'existe pas de $\lambda$ pour $\beta$.
                \item \cmoins Politiques déterministes $\rightarrow$ comportements extrêmes.
                % pas de formulation linéaire de la reward pour un budget donné
            \end{itemize}
        \end{alertblock}
        \begin{exampleblock}{Solution}
            Budgeted Reinforcement Learning.
            % Ce qui nous amène à la prochaine contribution: BRL
        \end{exampleblock}
    \end{frame}


    \begin{frame}{Cadre}
        \begin{block}{Processus de Decision Markovien (MDP)}
            \begin{itemize}
                \item $(\cS, \cA, P, R, \gamma)$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R(s_t, a_t)$ le retour des récompenses.
                \item Trouver $\pi^*$ t.q $\forall s\in\cS$:
                \begin{equation}
                    \label{eq:mdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA)^\cS} \expectedvalue[G_r^\pi | s_0=s]
                    \end{array}
                \end{equation}

            \end{itemize}
        \end{block}


        \begin{block}{}
            \begin{itemize}
                \item \cplus \textit{Tractable}
                \item \cmoins Comportements extrêmes
                \item \cmoins Calibration nécessaire (quel $\lambda$ pour $\beta$?)
                % A l ORAL si formulation sous contrainte
            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Cadre}

        \begin{block}{Processus de Decision Markovien \textcolor{purple}{sous Contrainte} (CMDP)}
            \begin{itemize}
                \item $(\cS, \cA, P, R,\textcolor{purple}{C}, \gamma,\textcolor{purple}{\beta})$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R(s_t, a_t)$ le retour des récompenses.
                \item \textcolor{purple}{ $G_c^\pi = \sum_{t=0}^\infty \gamma^t C(s_t, a_t)$ le retour des coûts.}
                \item Trouver $\pi^*$ t.q $\forall s\in\cS$:
                \begin{equation}
                    \label{eq:cmdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA)^\cS} \expectedvalue[G_r^\pi | s_0=s] \\
                        \text{ s.t. }  \textcolor{purple}{\expectedvalue[G_c^\pi | s_0=s] \leq \beta}
                    \end{array}
                \end{equation}
            \end{itemize}
        \end{block}

        \begin{block}{}
            \begin{itemize}
                \item \cplus \textit{Tractable}
                \item \cmean DOF supplémentaire (si mixture de politiques) % A L ORAL On peut définir un budget de safety , formuation plus naturelle
                \item \cmoins budget fixé
                % A L ORAL si le budget change on the fly, ou qu'il n'est pas adapté, on doit reapprendre une politique,  Or Le front de pareto est rarement linéaire, le choix du budget n'est pas évident


            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}{Cadre}

        \begin{block}{Processus de Decision Markovien \textcolor{purple}{Budgeté} (BMDP)}
            \begin{itemize}
                \item $(\cS, \cA, P, R,{C}, \gamma,\textcolor{purple}{\cB})$
                \item $G_r^\pi = \sum_{t=0}^\infty \gamma^t R(s_t, a_t)$ le retour des récompenses.
                \item  $G_c^\pi = \sum_{t=0}^\infty \gamma^t C(s_t, a_t)$ le retour des coûts.
                \item Trouver $\pi^*$ t.q $\forall (s,\textcolor{purple}{\beta})\in\cS\times\textcolor{purple}{\cB}$:
                \begin{equation}
                    \label{eq:bmdp}
                    \begin{array}{lcr}
                        \displaystyle \pi^* \in \argmax_{\pi\in\cM(\cA\times\textcolor{purple}{\cB})^{\cS\times\textcolor{purple}{\cB}}} \expectedvalue[G_r^\pi | s_0=s,\textcolor{purple}{\beta_0=\beta}] \\
                        \text{ s.t. }  \expectedvalue[G_c^\pi | s_0=s,\textcolor{purple}{\beta_0=\beta}] \leq \beta
                    \end{array}
                \end{equation}
            \end{itemize}
        \end{block}


        \begin{block}{}
            \begin{itemize}
                \item \cmoins \textit{Non Tractable}
                \item \cplus DOF supplémentaire
                \item \cplus budget non fixé

                % A L ORAL On peut définir un budget de safety et d'afranchir des lambda.}
            \end{itemize}
        \end{block}

    \end{frame}


    \begin{frame}{Cadre augmenté}

        \textbf{Politique budgeté} $\pi\in\Pi$
        \begin{itemize}
            \item $ \pi:\underbrace{(s,\beta)}_{\os} \rightarrow \underbrace{(a,\beta')}_{\oa}$
        \end{itemize}

        \textbf{Domain}
        \begin{itemize}
            \item Etats $\ocS = \cS\times\cB$.
            \item Actions $\ocA = \cA\times\cB$.
            \item Dynamique $\ov{P}$
            %$\left((s',\beta') \condbar (s,\beta), (a, \beta_a)\right) \eqdef P(s'|s, a)\delta(\beta' - \beta_a)$.
        \end{itemize}
        \textbf{2D signals}
        \begin{itemize}
            \item Récompenses $\ov{R} = (R, C)$
            \item Retours $G^\pi = (G_r^\pi, G_c^\pi)$
            \item $V^\pi(\os) = (V_r^\pi, V_c^\pi) \eqdef \expectedvalue\left[ G^\pi \condbar \ov{s_0} = \os\right]$
            \item $Q^\pi(\os, \oa)= (Q_r^\pi, Q_c^\pi) \eqdef \expectedvalue\left[ G^\pi \condbar \ov{s_0} = \os, \ov{a_0} = \oa\right]$
        \end{itemize}

    \end{frame}

    \begin{frame}{Optimalité}
        \begin{definition}
            \begin{enumerate}
                \item[(i)] \pause\colorbox{red}{Respecter le budget $\beta$}:
                \begin{equation*}
                    \Pi_a(\os) \eqdef \{\pi\in\Pi: V_c^\pi(s, \beta) \mathcolorbox{red}{\leq \beta}\}
                \end{equation*}
                \item[(ii)] \pause\colorbox{green}{Maximiser les récompenses}:
                \begin{equation*}
                    V_r^*(\os) \eqdef \mathcolorbox{green}{\max}_{\pi\in\Pi_a(\os)}  V_r^\pi(\os) \qquad\quad \Pi_r(\os) \eqdef \mathcolorbox{green}{\argmax}_{\pi\in\Pi_a(\os)}  V_r^\pi(\os)
                \end{equation*}
                \item[(iii)] \pause\colorbox{yellow}{Minimiser les coûts}:
                \begin{equation*}
                    V_c^*(\os) \eqdef \mathcolorbox{yellow}{\min}_{\pi\in\Pi_r(\os)}  V_c^\pi(\os), \qquad\quad \Pi^*(\os) \eqdef \mathcolorbox{yellow}{\argmin}_{\pi\in\Pi_r(\os)}  V_c^\pi(\os)
                \end{equation*}
            \end{enumerate}

            \pause On défini $Q^*$ de la même façon.
        \end{definition}
    \end{frame}

    \begin{frame}{Equation d'optimalité de Bellman}
        \begin{block}{Equation d'optimalité de Bellman}
            $Q^*$ vérifie:
            \begin{align*}
                Q^{*}(\os, \oa) &= \cT Q^{*}(\os, \oa)\\
                &\eqdef \ov{R}(\os, \oa) + \gamma \sum_{\os'\in\ocS} \ov{P}(\ov{s'} | \os, \oa)\sum_{\ov{a'}\in \ocA} \pi_\text{greedy}(\ov{a'}|\ov{s'}; Q^*) Q^{*}(\ov{s'}, \ov{a'}),
            \end{align*}
            avec:\pause
            \begin{align*}
                \pi_\text{greedy}(\cdot|\os; Q) \in &\mathcolorbox{yellow}{\argmin}_{\rho\in\Pi_r^Q} \sum_{\oa}\rho(\oa) Q_c(\os, \oa), \\
                \text{where }\quad \Pi_r^Q \eqdef &\mathcolorbox{green}{\argmax}_{\rho\in\cM(\ocA)} \sum_{\oa}\rho(\oa) Q_r(\os, \oa) \\
                & \text{ s.t. }   \sum_{\oa}\rho(\oa) Q_c(\os, \oa) \mathcolorbox{red}{\leq \beta}
            \end{align*}
        \end{block}
        % ORAL: comment résoudre ce problème ?
    \end{frame}


    \foreach \n in {0,1,2,3,4}{
    \begin{frame}{Résoudre le problème non linéaire}
        \begin{figure}
            \centering
            %%\vspace{-1.5em}
            \includegraphics[scale=1.0,page=1]{img/b\n}
        \end{figure}
    \end{frame}
    }

    \begin{frame}{Comportement asymptotique}


        \begin{alertblock}{ \cmoins $\cT$ n'est pas une contraction:}
            $\forall\epsilon>0, \exists Q^1,Q^2\in(\Real^2)^{\ocS\ocA}:\|\cT Q^1-\cT Q^2\|_\infty \geq \frac{1}{\epsilon}\|Q^1-Q^2\|_\infty$
        \end{alertblock}

        \begin{exampleblock}{\cplus sauf quand les $Q$-fonctions sont lisses:}
            $\cT$ est une contraction pour le sous-ensemble $\cL_\gamma$ des $Q$-fonctions tel que "$Q_r$ est $L$-Lipschitz par rapport $Q_c$", avec $L<\frac{1}{\gamma}-1$

        \end{exampleblock}
    \end{frame}

    \begin{frame}

        \begin{block}{Budgeted Value-Iteration}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q(\os,\oa) \leftarrow \bo Q(\os,\oa)\ \forall (\os,\oa)$ jusqu'à convergence % peut ne pas arriver
                \item Return $\pi(\os) = \pi_\text{greedy}(\cdot|\os; Q)$
            \end{itemize}
        \end{block}
        \pause
        \begin{alertblock}{}
            \begin{itemize}
                \item Comment calculer $\bo Q$ si $\transition$, $\reward$ et $\constraint$ sont inconnues?
                \begin{itemize}
                    \item Estimateur avec $\mathcal{D}=\{(\os_i,\oa_i,\ov{r}_i',\os_i')\}_{i\in[0,N]}$
                \end{itemize}
                \item Comment calculer $Q\ \forall (\os,\oa) \in \ocS\times\ocA$ si $\ocS$ est très grand voir continu ?
                \begin{itemize}
                    \item Approximation de fonction
                \end{itemize}
            \end{itemize}

        \end{alertblock}
        \pause
        \begin{block}{Budgeted Fitted-Q}
            \begin{itemize}
                \item $Q \leftarrow \mathbf{0}$
                \item $Q \leftarrow \Gamma(\{\os_{\indextransition},\oa_{\indextransition}\}_{{\indextransition}\in \T},\{\ov{r}'_{\indextransition} + \gamma \sum_{\ov{a}'\in \cA} \pi_\text{greedy}(\ov{a}'|\ov{s}'_{\indextransition}; Q) Q(\ov{s}'_{\indextransition}, \ov{a}')_{{\indextransition} \in \T}\})$
                \item Return $\pi(\os) = \pi_\text{greedy}(\cdot|\os; Q)$
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}{Exploration Risk-sensitive}

        Comment construire $\cD$?

        \begin{exampleblock}{Solution}
            Exploration avec Risk-sensitive $\epsilon$-greedy
        \end{exampleblock}
        $\rightarrow$ Pas abordée dans cette présentation
    \end{frame}

    \begin{frame}{Expériences}

        \begin{block}{Environnement}

            Remplissage d'un formulaire

            \begin{itemize}
                \item \texttt{ask-next}%: ask next slot (with ASR errors).
                \item \texttt{repeat-oral} (sécurisé + erreurs ASR)%: repeat current slot (with ASR errors).
                \item \texttt{repeat-numpad} (risqué)%: repeat current slot using numeric pad (without ASR errors).
                \item \texttt{summarize-inform}
                %: summarise slots values and return the form result. If values are correct, the dialogue ends successfully, if not, the slot values are reset and the dialogue continues from the first slot.
            \end{itemize}


        \end{block}
    \end{frame}

    \begin{frame}{Expériences}
        \begin{center}
            \includegraphics[width=0.75\textwidth]{img/slot-filling.pdf}
        \end{center}
    \end{frame}



    \begin{frame}{Pour aller plus loin}

        \begin{itemize}
            \item Les récompenses en dialogue sont très \textit{sparse}.
            \item Et si on utilisait une politique sécurisée pour découvrir plus de récompenses?
        \end{itemize}

        \begin{exampleblock}{Solution}
            Transfert avec $\epsilon$- safe
        \end{exampleblock}
        $\rightarrow$ Pas abordée dans cette présentation
    \end{frame}
    %    \subsection{$\epsilon$-safe}
    %    \begin{frame}{$\epsilon$- safe}
    %        Pas abordé dans cette présentation.
    %    \end{frame}

    \section{Conclusion et perspectives}
    \begin{frame}{Conclusion}
        \pause
        \begin{itemize}
            \item Prise de décision dans un SDS = le gestionnaire de dialogue
            \item Formulation avec du RL
            \pause
            \item Comment s'adapater rapidement à un nouvel utilisateur ?
            \begin{itemize}
                \item Apprentissage par transfert
            \end{itemize}
            \pause
            \item Deux directions:
            \pause
            \begin{itemize}
                \item Mise à l'échelle de solutions existantes
                \begin{itemize}
                    \item PD-distance: Clustering de politiques
                    \item TDQN: Solution de transfert en continu avec DQN (pas abordé ici)
                \end{itemize}
                \pause\item Approche sécurisée:
                \begin{itemize}
                    \item formulation lagragienne insuffisante $\rightarrow$ Budgeted-RL
                    \item $\epsilon$-safe (pas abordé ici)
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{frame}

    \begin{frame}{Perspectives}
        \begin{itemize}
            \item  Multi-agents
            \item Co-adaptation

            %            \item Nouvelles notions de sureté % induction en erreur,
        \end{itemize}

    \end{frame}

    %    \begin{frame}
    %        \printbibliography[heading=bibempty]
    %
    %    \end{frame}

\end{document}

